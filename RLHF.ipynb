{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "from typing import Iterable, Sequence, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from itertools import cycle\n",
    "from random import randint\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "set_seed(2024)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name=\"lvwerra/gpt2-imdb\"                          \n",
    "    reward_model_name=\"EKKam/opt350m_imdb_sentiment_reward\" #\"lvwerra/distilbert-imdb\"\n",
    "    seq_length= 1024    # 1024 compatible with GPT2, Distilbert supports 512\n",
    "    batch_size= 64      #128      # input to dataloader function\n",
    "    lr= 7.41e-6         #6.41e-6    #9.41e-6     #1.41e-5         #0.00006\n",
    "    prompt_size= 10     #8      #10     #30     # query length for the generator\n",
    "    epochs = 1          # number of epochs\n",
    "    mini_batch_size= 4      \n",
    "    gen_kwargs = {'max_new_tokens': 40,     # reduce if run crashes for OOM\n",
    "                  'top_k': 0,\n",
    "                  'top_p': 1.0,\n",
    "                  'do_sample': True\n",
    "                  }\n",
    "    kl_coef= 0.2    #0.01\n",
    "    gamma= 1\n",
    "    lam= 0.95\n",
    "    cliprange= 0.2\n",
    "    cliprange_value= 0.2\n",
    "    vf_coef= 0.0001\n",
    "    entropy_coef= 0.5\n",
    "\n",
    "\n",
    "args = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding_side=left for decoder-only architectures\n",
    "# https://discuss.huggingface.co/t/the-effect-of-padding-side/67188/4\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name,\n",
    "                                          padding_side='left')\n",
    "\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # CUDA error: device-side assert triggered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataset to be used for the training.\n",
    "It is a series of prompts, will be used to generate the responses and compute the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31eaa9c3888f432e8e9440e867f2aace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_dataset(args, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "\n",
    "    # load the IMDB dataset\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "\n",
    "    # Only choose reviews with more than 200 tokens\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 50, batched=False)\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        # From each review just keep the first `input_size` tokens, this represents the prompt used to generate the response\n",
    "        sample[\"input_ids\"] = tokenizer(sample[\"review\"], \n",
    "                                        padding='max_length', \n",
    "                                        max_length=args.prompt_size, \n",
    "                                        truncation=True).input_ids\n",
    "        sample[\"attention_mask\"] = torch.ones_like(torch.tensor(sample[\"input_ids\"]))\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "dataset = build_dataset(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(dataset, data_collator=None):\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=data_collator,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "#dataloader = iter(prepare_dataloader(dataset, data_collator=None))\n",
    "dataloader = cycle(prepare_dataloader(dataset, data_collator=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM as an RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, trainable=False):\n",
    "        super().__init__()\n",
    "        self.trainable = trainable\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Gen params during training overrides these settings\n",
    "        #self.model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        #self.model.generation_config.eos_token_id = None # disable `pad_token_id` and `eos_token_id` because we just want to\n",
    "        #self.model.generation_config.pad_token_id = None  # generate tokens without truncation / padding\n",
    "\n",
    "        if not self.trainable:\n",
    "            self.model = self.model.eval()\n",
    "            self.model.requires_grad_(False)\n",
    "        else:\n",
    "            n_embd = self.model.lm_head.in_features\n",
    "            num_labels = 1\n",
    "            self.value_head = nn.Sequential(\n",
    "                nn.LayerNorm(n_embd),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(n_embd, 4*n_embd),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4*n_embd, num_labels),\n",
    "            ).to(torch.bfloat16).to(self.model.device)\n",
    "        \n",
    "        self.logit_head = self.model.get_output_embeddings()\n",
    "\n",
    "    def generate(self, input_ids, **x):\n",
    "        return self.model.generate(input_ids, **x)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        lm_logits = self.logit_head(last_hidden_state)\n",
    "        if self.trainable:\n",
    "            value = self.value_head(last_hidden_state).squeeze(-1)\n",
    "            return lm_logits, value\n",
    "        else:\n",
    "            return lm_logits\n",
    "\n",
    "\n",
    "ref_model = Agent(trainable=False)\n",
    "\n",
    "model = Agent(trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobs_from_logits(logits, labels):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    logprobs_labels = torch.gather(logprobs, dim=-1, index=labels.unsqueeze(-1))\n",
    "    return logprobs_labels.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "     args.reward_model_name,\n",
    "     torch_dtype=torch.bfloat16,\n",
    "     device_map=\"auto\",\n",
    "     num_labels=1,\n",
    ")\n",
    "\n",
    "reward_model = reward_model.eval()\n",
    "reward_model = reward_model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(samples):\n",
    "    ins = tokenizer(samples, padding=True, truncation=True, max_length=args.seq_length, return_tensors='pt')\n",
    "    logits = reward_model(**ins.to(reward_model.device)).logits\n",
    "    temperature = 0.3\n",
    "    sentiments = torch.sigmoid(logits*temperature)[:,0].detach().cpu().tolist()\n",
    "    return sentiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPORLElement:\n",
    "    query_tensor: TensorType[\"query_size\"]\n",
    "    response_tensor: TensorType[\"response_size\"]\n",
    "    logprobs: TensorType[\"response_size\", \"vocab_size\"]\n",
    "    values: TensorType[\"response_size\"]\n",
    "    rewards: TensorType[\"response_size\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPORLBatch:\n",
    "    query_tensors: TensorType[\"batch_size\", \"query_size\"]\n",
    "    response_tensors: TensorType[\"batch_size\", \"response_size\"]\n",
    "    logprobs: TensorType[\"batch_size\", \"response_size\", \"vocab_size\"]\n",
    "    values: TensorType[\"batch_size\", \"response_size\"]\n",
    "    rewards: TensorType[\"batch_size\", \"response_size\"]\n",
    "\n",
    "\n",
    "def create_loader(elems, mini_batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    def collate_fn(elems: Iterable[PPORLElement]):\n",
    "        return PPORLBatch(\n",
    "            pad_sequence(\n",
    "                [elem.query_tensor for elem in elems],\n",
    "                padding_value=tokenizer.pad_token_id,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            pad_sequence(\n",
    "                [elem.response_tensor for elem in elems],\n",
    "                padding_value=tokenizer.pad_token_id,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            pad_sequence(\n",
    "                [elem.logprobs for elem in elems],\n",
    "                padding_value=tokenizer.pad_token_id,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            pad_sequence(\n",
    "                [elem.values for elem in elems],\n",
    "                padding_value=tokenizer.pad_token_id,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            pad_sequence(\n",
    "                [elem.rewards for elem in elems],\n",
    "                padding_value=tokenizer.pad_token_id,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return DataLoader(elems, mini_batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae(\n",
    "    values,\n",
    "    rewards,\n",
    "):\n",
    "    advantages = torch.zeros_like(rewards, device=rewards.device)\n",
    "    last_advantage = 0\n",
    "    last_value = 0\n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(rewards.shape[1])):\n",
    "            delta = rewards[:, t] + args.gamma * last_value - values[:, t]\n",
    "            last_advantage = delta + args.gamma * args.lam * last_advantage\n",
    "            advantages[:, t] = last_advantage\n",
    "            last_value = values[:, t]\n",
    "        returns = advantages + values\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/huggingface/trl/blob/main/trl/core.py\n",
    "\n",
    "def entropy_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Calculate entropy from logits.\"\"\"\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd * logits, axis=-1)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_mean: https://github.com/huggingface/trl/blob/main/trl/core.py\n",
    "def masked_mean(values: torch.Tensor, mask: torch.Tensor, axis: Optional[bool] = None) -> torch.Tensor:\n",
    "    \"\"\"Compute mean of tensor with a masked values.\"\"\"\n",
    "    if axis is not None:\n",
    "        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)\n",
    "    else:\n",
    "        return (values * mask).sum() / mask.sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(\n",
    "    logits,\n",
    "    logprobs,\n",
    "    values,\n",
    "    old_logprobs,\n",
    "    old_values,\n",
    "    advantages,\n",
    "    returns,\n",
    "    mask,\n",
    "):\n",
    "\n",
    "    ### Value Loss\n",
    "    values_clipped = torch.clamp(values,\n",
    "                                 old_values - args.cliprange_value,\n",
    "                                 old_values + args.cliprange_value,)\n",
    "    \n",
    "    n = mask.sum()\n",
    "    \n",
    "    # As per the paper without clipping\n",
    "    # vf_loss = 0.5 * torch.sum(vf_loss * mask) / n\n",
    "\n",
    "    vf_loss1 = (values - returns) ** 2\n",
    "    vf_loss2 = (values_clipped - returns) ** 2\n",
    "    #vf_loss = 0.5 * torch.sum(torch.max(vf_loss1, vf_loss2) * mask) / n\n",
    "    vf_loss = 0.5 * masked_mean(torch.max(vf_loss1, vf_loss2), mask)\n",
    "    vf_loss = args.vf_coef * vf_loss\n",
    "    #print(\"vf_loss: \", vf_loss)\n",
    "    \n",
    "\n",
    "    ### Entropy Loss\n",
    "    # The entropy to force the model to explore\n",
    "    # entropy_loss = torch.sum(entropy_from_logits(logits) * mask) / n\n",
    "    entropy_loss = masked_mean(entropy_from_logits(logits), mask)\n",
    "    entropy_loss = args.entropy_coef * entropy_loss\n",
    "    #print(\"entropy_loss: \", entropy_loss)\n",
    "\n",
    "\n",
    "    ### Policy Gradient Loss\n",
    "    # Ratio between the log probability of the new policy and the old policy\n",
    "    log_ratio = (logprobs - old_logprobs) * mask\n",
    "    ratio = torch.exp(log_ratio)\n",
    "    \n",
    "    # \"minus\" sign : to maximize the objective function since torch optimizer minimizes the loss by default\n",
    "    pg_loss1 = -advantages * ratio\n",
    "    pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)\n",
    "    pg_loss = masked_mean(torch.max(pg_loss1, pg_loss2), mask)\n",
    "    #pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / n\n",
    "    #print(\"policy loss: \", pg_loss)\n",
    "\n",
    "\n",
    "    ### Total Loss\n",
    "    loss = pg_loss + vf_loss + entropy_loss\n",
    "    \n",
    "\n",
    "    del(logits, logprobs, values, old_logprobs, old_values, advantages, returns, mask,\n",
    "        values_clipped, vf_loss1, vf_loss2, vf_loss, entropy_loss, pg_loss1, pg_loss2, pg_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(mini_batch):\n",
    "    query_tensors = mini_batch.query_tensors\n",
    "    response_tensors = mini_batch.response_tensors\n",
    "    old_logprobs = mini_batch.logprobs\n",
    "    old_values = mini_batch.values\n",
    "    old_rewards = mini_batch.rewards\n",
    "\n",
    "    response_length = old_rewards.shape[1]\n",
    "\n",
    "    advantages, returns = gae(old_values, old_rewards)\n",
    "\n",
    "    trajectories = torch.hstack([mini_batch.query_tensors, mini_batch.response_tensors])\n",
    "    attention_mask = trajectories.not_equal(tokenizer.pad_token_id).long()\n",
    "\n",
    "    logits, values_pred = model(trajectories, attention_mask=attention_mask)\n",
    "\n",
    "    values_pred = values_pred[:, :-1]\n",
    "    logprobs = logprobs_from_logits(logits[:, :-1, :], trajectories[:, 1:])\n",
    "    attention_mask = attention_mask[:, :-1]\n",
    "\n",
    "    start = query_tensors.shape[1] - 1\n",
    "    end = start + response_length\n",
    "    logits, logprobs, values_pred, mask = (\n",
    "        logits[:, start:end, :],\n",
    "        logprobs[:, start:end],\n",
    "        values_pred[:, start:end],\n",
    "        attention_mask[:, start:end],\n",
    "    )\n",
    "\n",
    "    loss = ppo_loss(\n",
    "        logits=logits,\n",
    "        logprobs=logprobs,\n",
    "        values=values_pred,\n",
    "        old_logprobs=old_logprobs,\n",
    "        old_values=old_values,\n",
    "        advantages=advantages,\n",
    "        returns=returns,\n",
    "        mask=mask,\n",
    "    )\n",
    "\n",
    "\n",
    "    del (query_tensors, response_tensors, old_logprobs, old_values, returns,\n",
    "         advantages, trajectories, attention_mask, logits, values_pred, logprobs)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    return loss, old_rewards[:,-1].mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference before alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = tokenizer(\n",
    "    [\"This is an action Western.\", \"I saw this movie recently because\"],\n",
    "    return_tensors='pt',\n",
    "    padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outs = model.generate(\n",
    "        ins['input_ids'].to(ref_model.model.device),\n",
    "        attention_mask=ins['attention_mask'].to(ref_model.model.device),\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=2.,\n",
    "    )\n",
    "\n",
    "for i in range(len(outs)):\n",
    "    generated_text = tokenizer.decode(outs[i], skip_special_tokens=True)\n",
    "    print(\"\\n\" + \"\\033[1;30m\" + generated_text)\n",
    "    print(\"\\033[1;32m\" +'Score: ', np.round(reward_fn([generated_text]), decimals=4)[0], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), args.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "iter_in_a_epoch = 380   # for batch_size = 64\n",
    "\n",
    "for iter, batch in tqdm(enumerate(dataloader)):\n",
    "\n",
    "    args.gen_kwargs['max_new_tokens'] = randint(8, 16)      #randint(20, 40)\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "            args.gen_kwargs,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    #### Phase 1: Get trajectories from the offline policy\n",
    "    query_tensors = batch[\"input_ids\"].to(model.model.device)\n",
    "    attention_mask = batch['attention_mask'].to(model.model.device)\n",
    "\n",
    "    trajectories = model.generate( query_tensors,\n",
    "                                attention_mask=attention_mask,\n",
    "                                **generate_kwargs)\n",
    "\n",
    "    response_tensors = trajectories[:, query_tensors.shape[1]:]\n",
    "\n",
    "    attention_mask = trajectories.not_equal(tokenizer.eos_token_id).long()\n",
    "\n",
    "    ref_model = ref_model.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        logits, values = model(\n",
    "            trajectories,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        ref_logits = ref_model(\n",
    "            trajectories,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "    ref_model = ref_model.to('cpu')\n",
    "\n",
    "    logprobs = logprobs_from_logits(logits[:, :-1, :], trajectories[:, 1:])\n",
    "    ref_logprobs = logprobs_from_logits(ref_logits[:, :-1, :], trajectories[:, 1:])\n",
    "    n_trajectories = trajectories.shape[0]\n",
    "    values = values[:, :-1]\n",
    "\n",
    "    start = batch['input_ids'].shape[1] - 1\n",
    "    ends = start + attention_mask[:, start:].sum(1)\n",
    "    truncated_values = [values[i, start : ends[i]] for i in range(n_trajectories)]\n",
    "    truncated_logprobs = [logprobs[i, start : ends[i]] for i in range(n_trajectories)]\n",
    "\n",
    "    del logits, ref_logits, values\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    texts = tokenizer.batch_decode(trajectories, skip_special_tokens=True)\n",
    "    trajectories = trajectories.detach().cpu()\n",
    "\n",
    "    reward_model= reward_model.to('cuda')\n",
    "    scores = reward_fn(texts)\n",
    "    reward_model = reward_model.to('cpu')\n",
    "    \n",
    "    kl_div = -args.kl_coef * (logprobs - ref_logprobs)\n",
    "    all_rewards = [None] * n_trajectories\n",
    "    for i in range(n_trajectories):\n",
    "        rs = kl_div[i][start : ends[i]]\n",
    "        rs[-1] += scores[i]\n",
    "        all_rewards[i] = rs\n",
    "\n",
    "    new_rollout = [\n",
    "        PPORLElement(\n",
    "            query_tensor=query_tensors[i],\n",
    "            response_tensor=response_tensors[i],\n",
    "            logprobs=truncated_logprobs[i],\n",
    "            values=truncated_values[i],\n",
    "            rewards=all_rewards[i],\n",
    "        )\n",
    "        for i in range(n_trajectories)\n",
    "    ]\n",
    "\n",
    "    score = torch.tensor(scores).mean().detach().cpu().item()\n",
    "\n",
    "    all_scores.append(score)\n",
    "\n",
    "    if (iter % 95 == 0): \n",
    "        epoch = iter // 380\n",
    "        rem = iter % 380\n",
    "        print(f\"Epoch: {epoch}, Iteration: {rem}, Reward: {score:.3f}\")\n",
    "        print(\"Length of trajectory: \", len(trajectories[5]))\n",
    "        print(\"Generated tokens: \", trajectories[0, start:])\n",
    "        print(\"=\"*90)\n",
    "\n",
    "    train_loader = create_loader(new_rollout, args.mini_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    del (query_tensors, response_tensors, attention_mask, trajectories, logprobs, \n",
    "         ref_logprobs, truncated_logprobs, truncated_values, texts, scores, kl_div,\n",
    "         all_rewards, new_rollout)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    #### Phase 2: loss calculation and PPO Update\n",
    "    for i, mini_batch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        loss, reward = loss_fn(mini_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "\n",
    "    del train_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if (iter / iter_in_a_epoch == args.epochs):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference after alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = tokenizer(\n",
    "    [\"This is an action Western.\", \"I saw this movie recently because\"],\n",
    "    return_tensors='pt',\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outs = model.generate(\n",
    "        ins['input_ids'].to(model.model.device),\n",
    "        attention_mask=ins['attention_mask'].to(model.model.device),\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=2.,\n",
    "    )\n",
    "\n",
    "    print(\"Length of the outputs: \", len(outs[0]), len(outs[1]))\n",
    "    print(\"Out-0 in encoded form:\", outs[0])\n",
    "    print(\"Out-1 in encoded form:\", outs[1])\n",
    "\n",
    "    for i in range(len(outs)):\n",
    "        generated_text = tokenizer.decode(outs[i], skip_special_tokens=True)\n",
    "        print(\"\\n\" + \"\\033[1;30m\" + generated_text)\n",
    "        print(\"\\033[1;32m\" +'Score: ', np.round(reward_fn([generated_text]), decimals=4)[0], \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. [Paper - Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n",
    "2. [TRL - PPO Trainer](https://github.com/huggingface/trl)\n",
    "3. [Reinforcement Learning from Human Feedback: From Zero to chatGPT](https://www.youtube.com/watch?v=2MBJOuVq380)\n",
    "4. [Coding chatGPT from Scratch | Lecture 3: Full Pipeline](https://www.youtube.com/watch?v=11M_kfuPJ5I)\n",
    "5. [Reinforcement Learning from Human Feedback explained with math derivations and the PyTorch code](https://www.youtube.com/watch?v=qGyFrqc34yc)\n",
    "6. [The N Implementation Details of RLHF with PPO](https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
